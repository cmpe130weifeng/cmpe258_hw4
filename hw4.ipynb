{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A - Do a cobe lab with all hyperparameters learning rate decay, dropout, batch norm"
      ],
      "metadata": {
        "id": "2VU6gK9d5o6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hint :https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist?hl=en#0"
      ],
      "metadata": {
        "id": "dZbuvwLT5hHd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TJuXbFj9GMhz"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "# EPOCHS = 10\n",
        "\n",
        "training_images_file   = 'gs://mnist-public/train-images-idx3-ubyte'\n",
        "training_labels_file   = 'gs://mnist-public/train-labels-idx1-ubyte'\n",
        "validation_images_file = 'gs://mnist-public/t10k-images-idx3-ubyte'\n",
        "validation_labels_file = 'gs://mnist-public/t10k-labels-idx1-ubyte'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, math, json, shutil, pprint\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import IPython.display as display\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xHOIzRR52Ok",
        "outputId": "32b904ab-8690-4c3e-fec8-bd22618e572e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "def read_label(tf_bytestring):\n",
        "    label = tf.io.decode_raw(tf_bytestring, tf.uint8)\n",
        "    label = tf.reshape(label, [])\n",
        "    label = tf.one_hot(label, 10)\n",
        "    return label\n",
        "  \n",
        "def read_image(tf_bytestring):\n",
        "    image = tf.io.decode_raw(tf_bytestring, tf.uint8)\n",
        "    image = tf.cast(image, tf.float32)/256.0\n",
        "    image = tf.reshape(image, [28*28])\n",
        "    return image\n",
        "  \n",
        "def load_dataset(image_file, label_file):\n",
        "    imagedataset = tf.data.FixedLengthRecordDataset(image_file, 28*28, header_bytes=16)\n",
        "    imagedataset = imagedataset.map(read_image, num_parallel_calls=16)\n",
        "    labelsdataset = tf.data.FixedLengthRecordDataset(label_file, 1, header_bytes=8)\n",
        "    labelsdataset = labelsdataset.map(read_label, num_parallel_calls=16)\n",
        "    dataset = tf.data.Dataset.zip((imagedataset, labelsdataset))\n",
        "    return dataset \n",
        "  \n",
        "def get_training_dataset(image_file, label_file, batch_size):\n",
        "    dataset = load_dataset(image_file, label_file)\n",
        "    dataset = dataset.cache()  # this small dataset can be entirely cached in RAM, for TPU this is important to get good performance from such a small dataset\n",
        "    dataset = dataset.shuffle(5000, reshuffle_each_iteration=True)\n",
        "    dataset = dataset.repeat() # Mandatory for Keras for now\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True) # drop_remainder is important on TPU, batch size must be fixed\n",
        "    dataset = dataset.prefetch(AUTO)  # fetch next batches while training on the current one (-1: autotune prefetch buffer size)\n",
        "    return dataset\n",
        "  \n",
        "def get_validation_dataset(image_file, label_file):\n",
        "    dataset = load_dataset(image_file, label_file)\n",
        "    dataset = dataset.cache() # this small dataset can be entirely cached in RAM, for TPU this is important to get good performance from such a small dataset\n",
        "    dataset = dataset.batch(10000, drop_remainder=True) # 10000 items in eval dataset, all in one batch\n",
        "    dataset = dataset.repeat() # Mandatory for Keras for now\n",
        "    return dataset\n",
        "\n",
        "# instantiate the datasets\n",
        "training_dataset = get_training_dataset(training_images_file, training_labels_file, BATCH_SIZE)\n",
        "validation_dataset = get_validation_dataset(validation_images_file, validation_labels_file)\n",
        "\n",
        "# For TPU, we will need a function that returns the dataset\n",
        "training_input_fn = lambda: get_training_dataset(training_images_file, training_labels_file, BATCH_SIZE)\n",
        "validation_input_fn = lambda: get_validation_dataset(validation_images_file, validation_labels_file)"
      ],
      "metadata": {
        "id": "R_tQpOpj52Ru"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential(\n",
        "  [\n",
        "      tf.keras.layers.Reshape(input_shape=(28*28,), target_shape=(28, 28, 1)),\n",
        "      \n",
        "      tf.keras.layers.Conv2D(kernel_size=3, filters=12, use_bias=False, padding='same'),\n",
        "      tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
        "      tf.keras.layers.Activation('relu'),\n",
        "      \n",
        "      tf.keras.layers.Conv2D(kernel_size=6, filters=24, use_bias=False, padding='same', strides=2),\n",
        "      tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
        "      tf.keras.layers.Activation('relu'),\n",
        "      \n",
        "      tf.keras.layers.Conv2D(kernel_size=6, filters=32, use_bias=False, padding='same', strides=2),\n",
        "      tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
        "      tf.keras.layers.Activation('relu'),\n",
        "      \n",
        "      tf.keras.layers.Flatten(),\n",
        "      \n",
        "      tf.keras.layers.Dense(200, use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
        "      tf.keras.layers.Activation('relu'),\n",
        "      \n",
        "      tf.keras.layers.Dropout(0.3),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# print model layers\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8F9fmc152YS",
        "outputId": "642e7b80-9dd0-416d-99c5-05a45fc6403e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape_1 (Reshape)         (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 28, 28, 12)        108       \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 28, 28, 12)       36        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 28, 28, 12)        0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 14, 14, 24)        10368     \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 14, 14, 24)       72        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 14, 14, 24)        0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 7, 7, 32)          27648     \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 7, 7, 32)         96        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 7, 7, 32)          0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1568)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 200)               313600    \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 200)              600       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 200)               0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                2010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 354,538\n",
            "Trainable params: 354,002\n",
            "Non-trainable params: 536\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lr decay function\n",
        "def lr_decay(epoch):\n",
        "  return 0.01 * math.pow(0.666, epoch)\n",
        "\n",
        "# lr schedule callback\n",
        "lr_decay_callback = tf.keras.callbacks.LearningRateScheduler(lr_decay, verbose=True)"
      ],
      "metadata": {
        "id": "DPxFWd9GA-TQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch = 60000//BATCH_SIZE  # 60,000 items in this dataset\n",
        "print(\"Steps per epoch: \", steps_per_epoch)\n",
        "\n",
        "history = model.fit(training_dataset, steps_per_epoch=steps_per_epoch, epochs=1,\n",
        "                    validation_data=validation_dataset, validation_steps=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0YGVqhdA-Wa",
        "outputId": "e1e66770-85c7-4b17-8b44-01ab87d916f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps per epoch:  937\n",
            "937/937 [==============================] - 92s 98ms/step - loss: 0.0572 - accuracy: 0.9829 - val_loss: 0.0530 - val_accuracy: 0.9854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8L8sQAYO52bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B"
      ],
      "metadata": {
        "id": "6mzTszb55qbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X8An5ILT5s5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g8nn_XY25s8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wN98SGQv5s_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nipJuwM-5tC3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}